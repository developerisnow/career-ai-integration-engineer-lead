# Sessions
# Build Automations SandrukAI.v2

---
### 23:14
````prompt
Так, у меня получается, после прослушивания, прочитывания всего этого текста, readme и шести пунктов, возникает много идей, пометок и такие мечты, где действительно, во-первых, не нужен инструмент Fibery, а нужен, наверное, GitHub Projects, там можно сделать все необходимые эпики, истории, таски, все необходимые view, kanban, gantt, группировки, parent, child и прочее. И действительно, если будет обрабатываться моя переписка, мой voice 24 на 7 и будет extraction, как там говорилось, это очень мне поможет. Вот эти фоновые обработки. В принципе, я заметил, что некоторые инструменты подобраны не оптимальным образом, потому что, видимо, агент, конечно же, не может учитывать весь мой контекст 3000 файлов, которые там, наверное, 10-50 миллионов токенов размером. Он не знает, что у меня, например, есть понимание по GTD, что очень важно плюс-минус использовать календарь. У меня есть Google Apps Script для Telegram, который стучит мне, ну, календарь мероприятия напоминается, Gmail точно так же, потому что вот Telegram кое-как я смотрю, а Gmail отдельное приложение либо календаря у меня постоянно забывается. Даже это бывает забывается, мне нужно буквально, чтобы на моем Garmin звенел будильник за 10 минут до мероприятия, вот какое-то такое физическое действие вибрировало. Ну ладно, это о другом. Я хотел обозначить лишь, что есть интеграция с Gmail и календарем. Через Telegram есть даже аналог Grafana, это Signes решение под ключ. Тоже там по всем моим софтам разным в супергруппы уведомлялки приходят. Плюс у меня есть моя система каналов в Telegram, где по моим там семи интересам я там публикую, к ним подвязаны группы, есть вот, ну, это уже известно, да, там. Если, конечно же, читал по поводу профессии там developer advocate и прочее, что community, вот я организовал на выходных по тому же Cloud Code. Воркшоп был от одного Александра из Калифорнии, который показывал свой продукт, автоматическая разработка софтвера через агентов, Cloud Code. Вот, также, ну, в общем, да, есть такие инструменты, есть просто моя группа с моими же Telegram аккаунтами. Да, у меня есть аккаунт иногда под разные домены, под Crypto, под AI, под там Relocate, Франция отдельный был, Канада, ну и так далее. Это когда я эти страны изучал для... Так, не буду обо всем, вот есть инструмент Toggle, он хорошо для меня работает, я понимаю, что если бы его связать с моим Garmin и Aura Ring, плюс дать канал агенту, который бы смотрел мой Telegram и прочее, и, в принципе, просто трекал этот Toggle за меня, мне... он удобен, так как там все нужные там инструменты, я адаптировал его по сферам жизни, проектам, типам активностей и так далее. В принципе, он неплох. И писать какое-то отдельное решение не очень понятно, у него достаточно хороший API и дает все нужные данные. Да, кстати, биометрию данных, как сон и прочие, тоже можно собирать из Aura. И Garmin. По поводу Voice 24x7, он вообще уже реализован, и в нем автоматически я просто, в принципе, могу одеть себе на воротник микрофон Lark M2S и включить. Его хватает на часов 8-10, у меня их было 2 штуки, но один потерял, но не суть. Допустим, куплю еще один, да. И буду пока что так вот. И я его включаю, он записывает, и я сделал через Android ACR 15-минутные аудио нарезки, и через SyncThing Android приложение синхронизируется с Mac в папочку, которая в то же время, кажется, если не ошибаюсь, в Nextcloud еще перpеливает. Нет, по-моему, я это убрал. Задвоение. Но не в этом суть. Суть дальше, что Hammerspoon со скриптом Lua делает автоматический запуск и подгрузку в Super Whisper, который бесплатно это транскрибирует достаточно хорошо и сохраняет сам себя. А потом у меня еще есть Obsidian скрипт, который дергает эти JSON-ы и складывает, условно говоря, в заметку одного дня Obsidian, ну, Markdown. Я это все думал когда-то глобально прикрутить к нормальной Postgres с PG-вектором, хотя мне, я вот сейчас порекомендовал решение о Chrome, не знаю, ну, как бы тут на границах. Я видел это как сначала сделать SQL плюс PG-вектор, а затем когда-то уже, когда появятся силы, возможность заняться Graph Database Neo4j, но это нужно для начала привести в порядок свою там структуру, entities, вот этот управляемый хаос или периодически пикируемый хаос системности, да, или цифровой сад, кто как назовет. Так вот, очень мне важно, да, мне нравится идея GitHub, вот этих агентов, которые будут, да, там трекать, а еще то, что они будут решать эти задачи сами в автономном режиме и периодически отстукивать, это прямо выглядит очень интересным и возможным, как я по сути сейчас в, там, без контейнера запускал, но можно в контейнерах вот эти все задачи и смотреть их прогресс. Что еще добавить? Ну, какие-то прочие кучу инструментов я не очень вижу смысла сейчас перечислять. Наверное, мне стоит еще поглядеть заметку. Пока что все. А, ну, в целом, ну, типа, какой следующий шаг и какая реальная польза и чего мне не хватало, вот, что я забыл, самое главное, это, ну, хотя бы вспомнил и назову в конце, это то, что по вот этим задачам, там, по двухнедельным спринтам, да, или фазам, как будто мне вот этот контекст проектов собирать, который я сейчас генерирую в большом количестве, общаясь по 10-15 часов с LLM, Large Language Model, через Cloud Code, Cursor. У Cursor есть плагин, и внутри каждого проекта можно по Маку искать. Папочка начинается .specs.story, получается, этот плагин сохраняет автоматом. Там уже можно экстрактить чисто промпты, потому что output иногда это слишком много. А важно, чтобы обучать, какой я есть, и Digital Second Brain Twin или Extension, да, необходим на меня, соответственно, на основе вот этого понимания, что я из себя выдаю, какую информацию, в каком стиле и так далее. Вот, по поводу Cloud Code, я и сам написал NPM-плагин, он у меня лежит в репозиториях, называется Cloud Code Exporter, вот, я его сегодня еще с тем же кодом дорабатывал, и, наверное, вот этот, ну, как бы, такой выдергиватель контекста и код, который будет класть это в Obsidian, я вообще думаю, действительно, будет пусть одна Mega Note, хотя есть сомнения, не уверен, потому что вот с моим Obsidian, когда много заметок, вот все потом теряешься, короче, пусть пока одна, потом агентами буду раскидывать по заголовкам, там, условно говоря, схлопнул, свернул, вообще не все удобно, в Markdown. А вот с другой стороны, я еще думал далее, что тоже нет смысла складировать вот этого кучу контента, еще там может быть, ну, промптов я имею в виду, да, в них иногда может быть копипаст, и стоит иногда, может быть, сразу вот этот процессинг делать, entities, actions, да, key ideas, есть у меня вот какие-то цели, goals, сделать агента, который у меня, собственно, есть в виде mods, profile, там вот в Cursor я сделал, я же прохожу курс по карьере, чтобы стать AI solution architect, AI integration engineer, и для этого мне нужно нарабатывать STAR cases, такой некий синдром самозванца, что в моменте не может даже вспомнить, что крутого ты делал, или как-то вот связано, структурировано это ответить для других людей, может быть, это из-за специфики ADHD, когда слишком много вот всего со всех сторон мыслей, идей, и ты вот таким потоком только перегрузишь, и это воспринимается с обратной стороны не как гиперсверхкомпетенция, да, а и действительно там один на миллион такой, да, или миллиард, а как что-то непонятное, ну его вот, и вернусь обратно, вот эти вот STAR cases, и прочие, вот эти вот все штуки делать, единственное, конечно же, это все может улетать как сквозь сито, все знания, которые не написаны от руки, не пропущены через себя, поэтому нужно, конечно, уделять время самому как-то, вот в режиме квиза, может, меня это опрашивал, диалогов, вот бесед, такого интертеймента, интересно, как бы, проходило, чтобы вот эти ключевые вещи, как бы, проговаривались, нарабатывались, то есть, когда я другому человеку перескажу, или когда даже сам вслух повторю, или как-то объясню другому, это лучше обрабатывать, плюс визуал, вот этот, как вот, у Gemini есть функция Canvas, и потом, поэтому Canvas можно сделать веб-сайт, прямо, вообще, у меня сразу огромные простые информации, я за секунду додумываю, понимаю, как-то вот скриша, Mermaid.js мне помогает, да, там, emoji, ну, ладно, это все Obsidian, стайл, но я вернусь обратно, да, вот эти extraction по entities, по actions, и вот по profiles, goals, какие вот карьеры, агенты отдельные, чтобы вот это следили и дорабатывали с разных, как это, плоскостей, да, так, это вот задача, получается, выдергивание этого контекста, как будто выглядит еще более первоочередно, потому что голоса у меня сейчас не так много общения, то есть у меня вот бывает разный период, а Telegram, TG, Tuprom, это еще стоит обозначить, тут тоже уже есть решение, уже есть скрипт, который вот это все экстрактит, сохраняет, периодически дергает контакты, там вот эта периодика нарушена, там, ну это недолго сейчас починить, с моими уже спустя 3-4 недели навыками и инструментом Cursor, но суть такая, да, что периодически всех, потому что люди же добавляются, еще 30 переписок, и если я дерну человека, который еще у меня нет в контактной книге, там, CSV, которая сохраняется, и по нему, по этому регистру идет, искать его там по alias и так далее, ну в коде так сделано, то получается, этот человек не найдется, вот, это я делал для удобства, чтобы он мог по имени, username, или ID, ну, короче, находить там соответствие, ладно, это уже какая-то мелочь, но суть такая, да, что этот TG Prompt работает, а вот Extraction, я делал даже уже скрипт, который выборку за день собирает, агрегирует, делал, там множество разных скриптов, вот это все надо сливать воедино, еще один такой вопрос, это как вот получается, наверное, под каждую вещь нужно создавать атомарные репозитории в папке Repositories у меня, да, иначе начнется вот это все, и потихоньку причесывать, когда все в одном, там намешанные, у меня есть там TG MCP, недоделанный, кстати, Assistant Telegram, это, которое я в партнерстве с Сергеем делал, ну, скажем так, разные части, там вот эту глобальную идею свою, Social AI, было даже письмо, можно посмотреть, в универ французский я подавал на PhD, если бы я делал, то есть, Social Second Brain, я там такие какие-то теги называл, там есть краткое описание, если я как и делал, вот так, и того, что я подсветил, что мне забирать контекст из Cursor, Cursor еще в идеале бы из всяких остальных, там, есть экспортеры, monkey Scripts у меня из ChatGPT, я им сейчас не пользуюсь, с Gemini, я с ним, им пользуюсь, но ручками надо, да, из Claude Desktop, я им пользуюсь, в браузере есть тоже кнопочка, ну, не знаю, не так часто им пользуюсь, по сути, сейчас все, когда можно отдать задачу Cursor, автономно, он ее решает до результата, то, как будто остальные инструменты становятся не сильно нужными, целый день им пользовался, из Telegram, да, надо забирать, и это я всегда, общение веду, voice 24 на 7, тоже нужно доделать, забрать, все эти наработки уже почти сделаны, нужно причесать, привести в порядок на отдельные подрепы, автоматизировать, и чтобы в цикле работали, ну, только вот, конечно, обработка, extraction, там, entities, нужно спроектировать, это я и хотел, собственно, сделать с AI-кой несколько дней назад, а сегодня просто решил сделать свой цифровой портрет и досье, и очень неплохо получилось, вот, ну, добавлю вот еще мысли и контекста, надеюсь, они сейчас смогут между строк дочитаться, и еще можно поизучать мои доки, и выпустить версию вторую этих документов, можно прямо, мне нравится вот этот подход по первой версии, и можно прямо под папочку subfolder сделать V1, туда это все положить, и сделать папочку V2, и туда это все положить, и в эту папку, может быть, вот этот как-то voice организовать, его, может быть, чуть-чуть пост обработку сделать, я вот сейчас даже сделаю сам папочку V1 и V2, и там уже будешь видеть.
````
---
